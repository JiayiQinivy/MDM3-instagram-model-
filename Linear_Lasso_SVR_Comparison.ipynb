{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Comparison: Lasso Regression vs Linear SVR\n",
    "\n",
    "## Research Objective\n",
    "This notebook compares **Lasso Regression** and **Linear Support Vector Regression (SVR)** for predicting Instagram users' perceived stress scores (`perceived_stress_score`).\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "### 1. Lasso Regression (Least Absolute Shrinkage and Selection Operator)\n",
    "- **Objective Function**: $\\min_{\\beta} \\frac{1}{2n}||y - X\\beta||_2^2 + \\alpha||\\beta||_1$\n",
    "- **Key Feature**: Automatic feature selection via L1 regularization\n",
    "\n",
    "### 2. Linear SVR (Support Vector Regression with Linear Kernel)\n",
    "- **Objective Function**: $\\min_{w,b} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^{n}(\\xi_i + \\xi_i^*)$\n",
    "- **Key Feature**: ε-insensitive loss function, robust to outliers\n",
    "\n",
    "### 3. Evaluation Metrics\n",
    "- **RMSE**: $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$\n",
    "- **R²**: $1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "- **MAE**: $\\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import LinearSVR  # Use LinearSVR instead of SVR for better memory efficiency\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import gc  # Garbage collection for memory management\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Define Features and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature list (20 features)\n",
    "FEATURES = [\n",
    "    'daily_active_minutes_instagram',\n",
    "    'likes_given_per_day',\n",
    "    'comments_written_per_day',\n",
    "    'time_on_feed_per_day',\n",
    "    'user_engagement_score',\n",
    "    'sessions_per_day',\n",
    "    'dms_received_per_week',\n",
    "    'ads_viewed_per_day',\n",
    "    'average_session_length_minutes',\n",
    "    'stories_viewed_per_day',\n",
    "    'posts_created_per_week',\n",
    "    'time_on_reels_per_day',\n",
    "    'time_on_messages_per_day',\n",
    "    'ads_clicked_per_day',\n",
    "    'time_on_explore_per_day',\n",
    "    'age',\n",
    "    'dms_sent_per_week',\n",
    "    'diet_quality',\n",
    "    'exercise_hours_per_week',\n",
    "    'reels_watched_per_day'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = ['diet_quality']\n",
    "TARGET = 'perceived_stress_score'\n",
    "\n",
    "print(f\"Total features: {len(FEATURES)}\")\n",
    "print(f\"Target: {TARGET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with memory optimization - use float32 instead of float64\n",
    "df = pd.read_csv('instagram_usage_lifestyle.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in diet_quality\n",
    "print(\"Unique values in diet_quality:\")\n",
    "print(df['diet_quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Preprocessing\n",
    "\n",
    "### 4.1 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for processing\n",
    "df_processed = df[FEATURES + [TARGET]].copy()\n",
    "\n",
    "# Ordinal encoding for diet_quality\n",
    "diet_quality_mapping = {\n",
    "    'Poor': 1,\n",
    "    'Fair': 2,\n",
    "    'Average': 2,\n",
    "    'Good': 3,\n",
    "    'Excellent': 4\n",
    "}\n",
    "\n",
    "df_processed['diet_quality_encoded'] = df_processed['diet_quality'].map(diet_quality_mapping)\n",
    "\n",
    "# Check for unmapped values\n",
    "if df_processed['diet_quality_encoded'].isna().sum() > 0:\n",
    "    print(\"Using LabelEncoder as fallback...\")\n",
    "    le = LabelEncoder()\n",
    "    df_processed['diet_quality_encoded'] = le.fit_transform(df_processed['diet_quality'].astype(str))\n",
    "    print(f\"Mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "else:\n",
    "    print(\"Ordinal encoding successful!\")\n",
    "\n",
    "# Update feature list\n",
    "FEATURES_ENCODED = [f if f != 'diet_quality' else 'diet_quality_encoded' for f in FEATURES]\n",
    "\n",
    "# Show encoding\n",
    "print(\"\\nEncoding result:\")\n",
    "print(df_processed[['diet_quality', 'diet_quality_encoded']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extract Features and Convert to float32 (Memory Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "X = df_processed[FEATURES_ENCODED].values.astype(np.float32)  # Use float32 to save memory\n",
    "y = df_processed[TARGET].values.astype(np.float32)\n",
    "\n",
    "# Handle missing values\n",
    "X = np.nan_to_num(X, nan=np.nanmedian(X, axis=0))\n",
    "y = np.nan_to_num(y, nan=np.nanmedian(y))\n",
    "\n",
    "print(f\"X shape: {X.shape}, dtype: {X.dtype}\")\n",
    "print(f\"y shape: {y.shape}, dtype: {y.dtype}\")\n",
    "print(f\"Memory for X: {X.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "# Clear unused dataframes\n",
    "del df, df_processed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test_scaled = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "print(\"Standardization completed!\")\n",
    "print(f\"Train mean: {X_train_scaled.mean():.6f}, std: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Lasso Regression Model\n",
    "\n",
    "### 5.1 Hyperparameter Tuning (Memory-Optimized)\n",
    "\n",
    "Using **3-fold CV** with reduced parameter grid to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced parameter grid for memory efficiency\n",
    "lasso_param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "# Create Lasso model\n",
    "lasso = Lasso(max_iter=1000, random_state=42, tol=1e-3)\n",
    "\n",
    "# Grid search with 3-fold CV (reduced from 5 to save memory)\n",
    "lasso_grid_search = GridSearchCV(\n",
    "    lasso, \n",
    "    lasso_param_grid, \n",
    "    cv=3,  # Reduced to 3-fold\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,  # Single job to reduce memory\n",
    "    return_train_score=False  # Don't store train scores\n",
    ")\n",
    "\n",
    "print(\"Training Lasso model...\")\n",
    "lasso_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nLasso Best Parameters:\")\n",
    "print(f\"  alpha: {lasso_grid_search.best_params_['alpha']}\")\n",
    "print(f\"  CV RMSE: {np.sqrt(-lasso_grid_search.best_score_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best Lasso model\n",
    "best_lasso = lasso_grid_search.best_estimator_\n",
    "\n",
    "# Clear grid search results to free memory\n",
    "del lasso_grid_search\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Linear SVR Model\n",
    "\n",
    "### 6.1 Hyperparameter Tuning (Memory-Optimized)\n",
    "\n",
    "Using **LinearSVR** instead of SVR with kernel='linear' for better memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced parameter grid\n",
    "svr_param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'epsilon': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Use LinearSVR (more memory efficient than SVR with linear kernel)\n",
    "svr = LinearSVR(max_iter=1000, random_state=42, tol=1e-3, dual=True)\n",
    "\n",
    "# Grid search with 3-fold CV\n",
    "svr_grid_search = GridSearchCV(\n",
    "    svr, \n",
    "    svr_param_grid, \n",
    "    cv=3,  # Reduced to 3-fold\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,  # Single job to reduce memory\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"Training Linear SVR model...\")\n",
    "svr_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nLinear SVR Best Parameters:\")\n",
    "print(f\"  C: {svr_grid_search.best_params_['C']}\")\n",
    "print(f\"  epsilon: {svr_grid_search.best_params_['epsilon']}\")\n",
    "print(f\"  CV RMSE: {np.sqrt(-svr_grid_search.best_score_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best SVR model\n",
    "best_svr = svr_grid_search.best_estimator_\n",
    "\n",
    "# Clear grid search results\n",
    "del svr_grid_search\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Evaluation\n",
    "\n",
    "### 7.1 Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Evaluate model performance.\"\"\"\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Train_RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'Train_R2': r2_score(y_train, y_train_pred),\n",
    "        'Test_RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'Test_R2': r2_score(y_test, y_test_pred),\n",
    "        'Test_MAE': mean_absolute_error(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    return results, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "lasso_results, lasso_pred = evaluate_model(\n",
    "    best_lasso, X_train_scaled, X_test_scaled, y_train, y_test, 'Lasso'\n",
    ")\n",
    "\n",
    "svr_results, svr_pred = evaluate_model(\n",
    "    best_svr, X_train_scaled, X_test_scaled, y_train, y_test, 'Linear SVR'\n",
    ")\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame([lasso_results, svr_results])\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(\"=\" * 70)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Cross-Validation (5-Fold, Memory-Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale full data for CV\n",
    "scaler_full = StandardScaler()\n",
    "X_scaled = scaler_full.fit_transform(X).astype(np.float32)\n",
    "\n",
    "# 5-fold CV for final evaluation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lasso CV\n",
    "lasso_cv_model = Lasso(alpha=best_lasso.alpha, max_iter=1000, random_state=42)\n",
    "lasso_cv_scores = cross_val_score(lasso_cv_model, X_scaled, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "lasso_cv_rmse = np.sqrt(-lasso_cv_scores)\n",
    "\n",
    "# Linear SVR CV\n",
    "svr_cv_model = LinearSVR(C=best_svr.C, epsilon=best_svr.epsilon, max_iter=1000, random_state=42)\n",
    "svr_cv_scores = cross_val_score(svr_cv_model, X_scaled, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "svr_cv_rmse = np.sqrt(-svr_cv_scores)\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Lasso RMSE:      {lasso_cv_rmse.mean():.4f} ± {lasso_cv_rmse.std():.4f}\")\n",
    "print(f\"Linear SVR RMSE: {svr_cv_rmse.mean():.4f} ± {svr_cv_rmse.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "models = ['Lasso', 'Linear SVR']\n",
    "colors = ['steelblue', 'coral']\n",
    "\n",
    "# RMSE comparison\n",
    "rmse_means = [lasso_cv_rmse.mean(), svr_cv_rmse.mean()]\n",
    "rmse_stds = [lasso_cv_rmse.std(), svr_cv_rmse.std()]\n",
    "axes[0].bar(models, rmse_means, yerr=rmse_stds, color=colors, capsize=10, edgecolor='black')\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_title('RMSE Comparison (5-Fold CV)', fontsize=14)\n",
    "for i, (m, s) in enumerate(zip(rmse_means, rmse_stds)):\n",
    "    axes[0].text(i, m + s + 0.01, f'{m:.4f}', ha='center', fontsize=11)\n",
    "\n",
    "# R² comparison\n",
    "r2_values = [lasso_results['Test_R2'], svr_results['Test_R2']]\n",
    "axes[1].bar(models, r2_values, color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('R²', fontsize=12)\n",
    "axes[1].set_title('R² Comparison (Test Set)', fontsize=14)\n",
    "for i, v in enumerate(r2_values):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Predicted vs Actual Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Lasso\n",
    "axes[0].scatter(y_test, lasso_pred, alpha=0.3, color='steelblue', s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted', fontsize=12)\n",
    "axes[0].set_title(f'Lasso (R²={lasso_results[\"Test_R2\"]:.4f})', fontsize=14)\n",
    "\n",
    "# Linear SVR\n",
    "axes[1].scatter(y_test, svr_pred, alpha=0.3, color='coral', s=10)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted', fontsize=12)\n",
    "axes[1].set_title(f'Linear SVR (R²={svr_results[\"Test_R2\"]:.4f})', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature coefficients\n",
    "lasso_coef = best_lasso.coef_\n",
    "svr_coef = best_svr.coef_\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': FEATURES_ENCODED,\n",
    "    'Lasso_Coef': lasso_coef,\n",
    "    'SVR_Coef': svr_coef,\n",
    "    'Lasso_Abs': np.abs(lasso_coef),\n",
    "    'SVR_Abs': np.abs(svr_coef)\n",
    "}).sort_values('Lasso_Abs', ascending=False)\n",
    "\n",
    "print(\"Feature Coefficients (sorted by Lasso importance):\")\n",
    "print(\"=\" * 70)\n",
    "importance_df[['Feature', 'Lasso_Coef', 'SVR_Coef']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso feature selection\n",
    "selected = importance_df[importance_df['Lasso_Coef'] != 0]\n",
    "excluded = importance_df[importance_df['Lasso_Coef'] == 0]\n",
    "\n",
    "print(f\"\\nLasso selected {len(selected)}/{len(FEATURES_ENCODED)} features\")\n",
    "if len(excluded) > 0:\n",
    "    print(f\"\\nExcluded features (coefficient = 0):\")\n",
    "    for f in excluded['Feature'].values:\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature coefficient visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Lasso coefficients\n",
    "lasso_sorted = importance_df.sort_values('Lasso_Abs', ascending=True)\n",
    "colors_l = ['steelblue' if c >= 0 else 'coral' for c in lasso_sorted['Lasso_Coef']]\n",
    "axes[0].barh(lasso_sorted['Feature'], lasso_sorted['Lasso_Coef'], color=colors_l)\n",
    "axes[0].set_xlabel('Coefficient')\n",
    "axes[0].set_title('Lasso Feature Coefficients')\n",
    "axes[0].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "# SVR coefficients\n",
    "svr_sorted = importance_df.sort_values('SVR_Abs', ascending=True)\n",
    "colors_s = ['steelblue' if c >= 0 else 'coral' for c in svr_sorted['SVR_Coef']]\n",
    "axes[1].barh(svr_sorted['Feature'], svr_sorted['SVR_Coef'], color=colors_s)\n",
    "axes[1].set_xlabel('Coefficient')\n",
    "axes[1].set_title('Linear SVR Feature Coefficients')\n",
    "axes[1].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "final_summary = pd.DataFrame({\n",
    "    'Model': ['Lasso', 'Linear SVR'],\n",
    "    'Best_Params': [\n",
    "        f\"alpha={best_lasso.alpha}\",\n",
    "        f\"C={best_svr.C}, eps={best_svr.epsilon}\"\n",
    "    ],\n",
    "    'Test_RMSE': [lasso_results['Test_RMSE'], svr_results['Test_RMSE']],\n",
    "    'Test_R2': [lasso_results['Test_R2'], svr_results['Test_R2']],\n",
    "    'CV_RMSE_Mean': [lasso_cv_rmse.mean(), svr_cv_rmse.mean()],\n",
    "    'CV_RMSE_Std': [lasso_cv_rmse.std(), svr_cv_rmse.std()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "final_summary.to_csv('linear_models_results.csv', index=False)\n",
    "importance_df.to_csv('feature_coefficients.csv', index=False)\n",
    "\n",
    "print(\"\\nResults exported:\")\n",
    "print(\"  ✓ linear_models_results.csv\")\n",
    "print(\"  ✓ feature_coefficients.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusions\n",
    "\n",
    "### Memory Optimization Techniques Used:\n",
    "1. **float32** instead of float64 (50% memory reduction)\n",
    "2. **3-fold CV** for hyperparameter tuning (reduced from 5)\n",
    "3. **LinearSVR** instead of SVR with linear kernel (more efficient)\n",
    "4. **Reduced parameter grid** (fewer combinations)\n",
    "5. **Single-threaded** execution (n_jobs=1)\n",
    "6. **Garbage collection** after each step\n",
    "\n",
    "### Model Comparison:\n",
    "| Method | Advantages | Limitations |\n",
    "|--------|------------|-------------|\n",
    "| Lasso | Automatic feature selection | May exclude correlated features |\n",
    "| Linear SVR | Robust to outliers | No feature selection |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
